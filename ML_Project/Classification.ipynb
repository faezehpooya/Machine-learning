{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\fatemeh\\anaconda2\\envs\\py36\\lib\\site-packages (1.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\fatemeh\\anaconda2\\envs\\py36\\lib\\site-packages (from xgboost) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\fatemeh\\anaconda2\\envs\\py36\\lib\\site-packages (from xgboost) (1.5.2)\n",
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.7.0-py3-none-any.whl (167 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\fatemeh\\anaconda2\\envs\\py36\\lib\\site-packages (from imbalanced-learn->imblearn) (0.17.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\fatemeh\\anaconda2\\envs\\py36\\lib\\site-packages (from imbalanced-learn->imblearn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\fatemeh\\anaconda2\\envs\\py36\\lib\\site-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.23 in c:\\users\\fatemeh\\anaconda2\\envs\\py36\\lib\\site-packages (from imbalanced-learn->imblearn) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\fatemeh\\anaconda2\\envs\\py36\\lib\\site-packages (from scikit-learn>=0.23->imbalanced-learn->imblearn) (2.1.0)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.7.0 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(data, columns, scaler):\n",
    "    for col in columns:\n",
    "        data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
    "    return data\n",
    "  \n",
    "def one_hot_obj_feature(df,features):\n",
    "    new_df=pd.get_dummies(df,columns=features,sparse=True)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv(\"MLFinalProjectDataset/train_data.csv\")\n",
    "train_df = all_df.sample(frac=0.01, random_state=42)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "df=scale_data(train_df,list(train_df.columns)[:-1], preprocessing.MaxAbsScaler())\n",
    "test_df = all_df.drop(df.index).sample(frac=0.02, random_state=42)\n",
    "test_df=scale_data(test_df,list(test_df.columns)[:-1], preprocessing.MaxAbsScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class balance:\n",
    "    def __init__(self,x,y,model):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.model=model\n",
    "    def fit(self):\n",
    "        return self.model.fit_resample(self.x,self.y)\n",
    "\n",
    "class Classifier:\n",
    "    \"\"\"\n",
    "        train_data: tuple(train_features, train_tags)\n",
    "        test_data: tuple(test_features)\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, model):\n",
    "        self.train_sparse_matrix = x\n",
    "        self.train_tags = y\n",
    "        self.model = model\n",
    "        self.is_learned = False\n",
    "\n",
    "    def fit(self):\n",
    "        self.model.fit(self.train_sparse_matrix, self.train_tags)\n",
    "        self.is_learned = True\n",
    "\n",
    "    def predict(self,t):\n",
    "        if not self.is_learned:\n",
    "            self.fit()\n",
    "        return self.model.predict(t)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "def four_fold_CV(X, Y, fold_num=4):\n",
    "    \n",
    "    validation_sets = [None for i in range(fold_num)]\n",
    "    train_sets = [None for i in range(fold_num)] \n",
    "    \n",
    "    skf_cv = StratifiedKFold(n_splits=fold_num)\n",
    "    \n",
    "    i = 0\n",
    "    for train_idx, valid_idx in skf_cv.split(X, Y):\n",
    "        validation_sets[i] = {'X': X[valid_idx], 'Y': Y[valid_idx]}\n",
    "        train_sets[i] = {'X': X[train_idx], 'Y': Y[train_idx]}\n",
    "        i += 1\n",
    "    \n",
    "    return train_sets, validation_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x=df.drop(\"clicked\",axis=1)\n",
    "df_y=df[\"clicked\"]\n",
    "rus = balance(df_x,df_y,RandomUnderSampler(random_state=42, replacement=True))# fit predictor and target variable\n",
    "x_rus, y_rus = rus.fit()\n",
    "ros=balance(df_x,df_y,RandomOverSampler(random_state=42))\n",
    "x_ros, y_ros=ros.fit()\n",
    "smote = balance(df_x,df_y,SMOTE())\n",
    "x_smote, y_smote=smote.fit()\n",
    "nm = balance(df_x,df_y,NearMiss())\n",
    "x_nm,y_nm=nm.fit()\n",
    "\n",
    "x_rus, y_rus=x_rus.to_numpy(), y_rus.to_numpy()\n",
    "x_ros, y_ros=x_ros.to_numpy(), y_ros.to_numpy()\n",
    "x_smote, y_smote=x_smote.to_numpy(), y_smote.to_numpy()\n",
    "x_nm,y_nm=x_nm.to_numpy(),y_nm.to_numpy()\n",
    "\n",
    "balanced_all_fetuers=[x_rus, y_rus,x_ros, y_ros,x_smote, y_smote,x_nm,y_nm]\n",
    "method=[\"rus\",\"ros\",\"smote\",\"nm\"]\n",
    "\n",
    "df_x=test_df.drop(\"clicked\",axis=1)\n",
    "df_y=test_df[\"clicked\"]\n",
    "rus = balance(df_x,df_y,RandomUnderSampler(random_state=42, replacement=True))# fit predictor and target variable\n",
    "x_rus, y_rus = rus.fit()\n",
    "ros=balance(df_x,df_y,RandomOverSampler(random_state=42))\n",
    "x_ros, y_ros=ros.fit()\n",
    "smote = balance(df_x,df_y,SMOTE())\n",
    "x_smote, y_smote=smote.fit()\n",
    "nm = balance(df_x,df_y,NearMiss())\n",
    "x_nm,y_nm=nm.fit()\n",
    "\n",
    "x_rus, y_rus=x_rus.to_numpy(), y_rus.to_numpy()\n",
    "x_ros, y_ros=x_ros.to_numpy(), y_ros.to_numpy()\n",
    "x_smote, y_smote=x_smote.to_numpy(), y_smote.to_numpy()\n",
    "x_nm,y_nm=x_nm.to_numpy(),y_nm.to_numpy()\n",
    "\n",
    "test_balanced_all_fetuers=[x_rus, y_rus,x_ros, y_ros,x_smote, y_smote,x_nm,y_nm]\n",
    "method=[\"rus\",\"ros\",\"smote\",\"nm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mask = ['hourOfDay', 'creativeId', 'publisher', 'widgetId', 'device']\n",
    "\n",
    "df_x=df[features_mask]\n",
    "df_y=df[\"clicked\"]\n",
    "rus = balance(df_x,df_y,RandomUnderSampler(random_state=42, replacement=True))# fit predictor and target variable\n",
    "x_rus, y_rus = rus.fit()\n",
    "ros=balance(df_x,df_y,RandomOverSampler(random_state=42))\n",
    "x_ros, y_ros=ros.fit()\n",
    "smote = balance(df_x,df_y,SMOTE())\n",
    "x_smote, y_smote=smote.fit()\n",
    "nm = balance(df_x,df_y,NearMiss())\n",
    "x_nm,y_nm=nm.fit()\n",
    "\n",
    "x_rus, y_rus=x_rus.to_numpy(), y_rus.to_numpy()\n",
    "x_ros, y_ros=x_ros.to_numpy(), y_ros.to_numpy()\n",
    "x_smote, y_smote=x_smote.to_numpy(), y_smote.to_numpy()\n",
    "x_nm,y_nm=x_nm.to_numpy(),y_nm.to_numpy()\n",
    "\n",
    "balanced=[x_rus, y_rus,x_ros, y_ros,x_smote, y_smote,x_nm,y_nm]\n",
    "method=[\"rus\",\"ros\",\"smote\",\"nm\"]\n",
    "\n",
    "\n",
    "df_x=test_df[features_mask]\n",
    "df_y=test_df[\"clicked\"]\n",
    "rus = balance(df_x,df_y,RandomUnderSampler(random_state=42, replacement=True))# fit predictor and target variable\n",
    "x_rus, y_rus = rus.fit()\n",
    "ros=balance(df_x,df_y,RandomOverSampler(random_state=42))\n",
    "x_ros, y_ros=ros.fit()\n",
    "smote = balance(df_x,df_y,SMOTE())\n",
    "x_smote, y_smote=smote.fit()\n",
    "nm = balance(df_x,df_y,NearMiss())\n",
    "x_nm,y_nm=nm.fit()\n",
    "\n",
    "x_rus, y_rus=x_rus.to_numpy(), y_rus.to_numpy()\n",
    "x_ros, y_ros=x_ros.to_numpy(), y_ros.to_numpy()\n",
    "x_smote, y_smote=x_smote.to_numpy(), y_smote.to_numpy()\n",
    "x_nm,y_nm=x_nm.to_numpy(),y_nm.to_numpy()\n",
    "\n",
    "test_balanced=[x_rus, y_rus,x_ros, y_ros,x_smote, y_smote,x_nm,y_nm]\n",
    "method=[\"rus\",\"ros\",\"smote\",\"nm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CC = [0.01, 0.1, 1, 10]\n",
    "\n",
    "accs = [[0 for i in range(len(CC))]for j in range(4)]\n",
    "folds_num = 3\n",
    "for j in range(0,8,2):\n",
    "    train_sets, validation_sets = four_fold_CV(balanced_all_fetuers[j], balanced_all_fetuers[j+1], folds_num)\n",
    "    for (cntC, C) in enumerate(CC):\n",
    "        avg_acc = 0\n",
    "        for i in range(folds_num):\n",
    "            try:\n",
    "                t_X = train_sets[i]['X']\n",
    "                t_Y = train_sets[i]['Y']\n",
    "                v_X = validation_sets[i]['X']\n",
    "                v_Y = validation_sets[i]['Y']\n",
    "                clf=Classifier(t_X, t_Y,svm.SVC(C=C))\n",
    "                clf.fit()\n",
    "                avg_acc+=f1_score(v_Y,clf.predict(v_X))\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "        accs[j//2][cntC] = avg_acc / folds_num\n",
    "        print('-> mean f1-score 3-fold cross validation for svm with method', method[j//2], \n",
    "              'and c =', C, 'is:', accs[j//2][cntC])\n",
    "\n",
    "\n",
    "print(accs)\n",
    "print(\"--------------\")\n",
    "print(\"-> Result of 4-fold cross validation for svm :\")\n",
    "maxx=0\n",
    "maxx_i=-1\n",
    "maxx_j=-1\n",
    "for i in range(len(accs)):\n",
    "    for j in range(len(accs[i])):\n",
    "        if accs[i][j]>maxx:\n",
    "            maxx=accs[i][j]\n",
    "            maxx_i=i\n",
    "            maxx_j=j\n",
    "best_C = CC[maxx_j]\n",
    "print(\"    Best parameter C is:\", best_C, \"\\n\", \n",
    "      \"    with method:\",method[maxx_i],\"best accuracy =\",maxx)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5712708542821483, 0.5852227813039433, 0.5686469184117325, 0.564212325112828], [0.5922745341876631, 0.5680190312690726, 0.5657780601572795, 0.5673122885266936], [0.5783078723312106, 0.5635324095032397, 0.562799734228895, 0.5663391214259098], [0.587207788998157, 0.5721716413198905, 0.5958625613146281, 0.6339787684992702]]\n",
      "--------------\n",
      "-> Result of 4-fold cross validation for svm :\n",
      "    Best parameter C is: 10 \n",
      "     with method: nm best accuracy = 0.6339787684992702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CC = [0.01, 0.1, 1, 10]\n",
    "\n",
    "accs = [[0 for i in range(len(CC))]for j in range(4)]\n",
    "folds_num = 3\n",
    "for j in range(0,8,2):\n",
    "    train_sets, validation_sets = four_fold_CV(balanced[j], balanced[j+1], folds_num)\n",
    "    for (cntC, C) in enumerate(CC):\n",
    "        avg_acc = 0\n",
    "        for i in range(folds_num):\n",
    "            try:\n",
    "                t_X = train_sets[i]['X']\n",
    "                t_Y = train_sets[i]['Y']\n",
    "                v_X = validation_sets[i]['X']\n",
    "                v_Y = validation_sets[i]['Y']\n",
    "                clf=Classifier(t_X, t_Y,svm.SVC(C=C))\n",
    "                clf.fit()\n",
    "                avg_acc+=f1_score(v_Y,clf.predict(v_X))\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "        accs[j//2][cntC] = avg_acc / folds_num\n",
    "        print('-> mean f1-score 3-fold cross validation for svm with method', method[j//2], \n",
    "              'and c =', C, 'is:', accs[j//2][cntC])\n",
    "\n",
    "\n",
    "print(accs)\n",
    "print(\"--------------\")\n",
    "print(\"-> Result of 3-fold cross validation for svm with our selected features:\")\n",
    "maxx=0\n",
    "maxx_i=-1\n",
    "maxx_j=-1\n",
    "for i in range(len(accs)):\n",
    "    for j in range(len(accs[i])):\n",
    "        if accs[i][j]>maxx:\n",
    "            maxx=accs[i][j]\n",
    "            maxx_i=i\n",
    "            maxx_j=j\n",
    "best_C = CC[maxx_j]\n",
    "print(\"    Best parameter C is:\", best_C, \"\\n\", \n",
    "      \"    with method:\",method[maxx_i],\"best accuracy =\",maxx)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[0.5967004636350872, 0.5815487862528559, 0.576551879415649, 0.5832034167891991], [0.5867981358260647, 0.5886376634133571, 0.6215402672305637, 0.6188350982884393], [0.5840365963807864, 0.5776309943579285, 0.6121896182154699, 0.6137464488311024], [0.5810374724881552, 0.5895009359360274, 0.5971509121775429, 0.6156927447631586]]\n",
    "--------------\n",
    "-> Result of 4-fold cross validation for svm :\n",
    "    Best parameter C is: 1 \n",
    "     with method: ros best accuracy = 0.6215402672305637\n",
    "     \n",
    " [[0.5768446195566149, 0.5892935456419133, 0.5853994730043791, 0.582733900598742], [0.5863684344982306, 0.595372703828982, 0.620554149760623, 0.619595100260067], [0.5852592874068079, 0.5942046945291757, 0.6222750371705171, 0.6259665976908928], [0.555210945176802, 0.5799017394395695, 0.627717621110346, 0.6626145346281342]]\n",
    "--------------\n",
    "-> Result of 4-fold cross validation for svm :\n",
    "    Best parameter C is: 10 \n",
    "     with method: nm best accuracy = 0.6626145346281342"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method rus and c = 0.01 is: 0.5497463459353171\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method rus and c = 0.1 is: 0.5605014170843892\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method rus and c = 1 is: 0.5630897608005667\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method rus and c = 10 is: 0.5618140097893303\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method ros and c = 0.01 is: 0.5754330373811306\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method ros and c = 0.1 is: 0.5796980084906229\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method ros and c = 1 is: 0.5798789476426768\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method ros and c = 10 is: 0.58002461750957\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method smote and c = 0.01 is: 0.5750600864538804\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method smote and c = 0.1 is: 0.5759087977688392\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method smote and c = 1 is: 0.5751563526150727\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method smote and c = 10 is: 0.5749241923084834\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method nm and c = 0.01 is: 0.6205517405813097\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method nm and c = 0.1 is: 0.6257119325306431\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method nm and c = 1 is: 0.6205154389955232\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method nm and c = 10 is: 0.6201800531961721\n",
      "[[0.5497463459353171, 0.5605014170843892, 0.5630897608005667, 0.5618140097893303], [0.5754330373811306, 0.5796980084906229, 0.5798789476426768, 0.58002461750957], [0.5750600864538804, 0.5759087977688392, 0.5751563526150727, 0.5749241923084834], [0.6205517405813097, 0.6257119325306431, 0.6205154389955232, 0.6201800531961721]]\n",
      "--------------\n",
      "-> Result of 4-fold cross validation for logistic regression:\n",
      "    Best parameter C is: 0.1 \n",
      "     with method: nm best f1-score = 0.6257119325306431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CC = [0.01, 0.1, 1, 10]\n",
    "\n",
    "accs = [[0 for i in range(len(CC))]for j in range(4)]\n",
    "folds_num = 4\n",
    "for j in range(0,8,2):\n",
    "    train_sets, validation_sets = four_fold_CV(balanced_all_fetuers[j], balanced_all_fetuers[j+1], folds_num)\n",
    "\n",
    "    for (cntC, C) in enumerate(CC):\n",
    "        avg_acc = 0\n",
    "        for i in range(folds_num):\n",
    "            try:\n",
    "                t_X = train_sets[i]['X']\n",
    "                t_Y = train_sets[i]['Y']\n",
    "                v_X = validation_sets[i]['X']\n",
    "                v_Y = validation_sets[i]['Y']\n",
    "                clf=Classifier(t_X, t_Y,LogisticRegression(random_state=0,C=C, max_iter=10000))\n",
    "                clf.fit()\n",
    "                avg_acc+=f1_score(v_Y,clf.predict(v_X))\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "        accs[j//2][cntC] = avg_acc / folds_num\n",
    "        print('-> mean f1-score 4-fold cross validation for LogisticRegression with method', method[j//2], \n",
    "              'and c =', C, 'is:', accs[j//2][cntC])\n",
    "\n",
    "\n",
    "print(accs)\n",
    "print(\"--------------\")\n",
    "print(\"-> Result of 4-fold cross validation for logistic regression:\")\n",
    "maxx=0\n",
    "maxx_i=-1\n",
    "maxx_j=-1\n",
    "for i in range(len(accs)):\n",
    "    for j in range(len(accs[i])):\n",
    "        if accs[i][j]>maxx:\n",
    "            maxx=accs[i][j]\n",
    "            maxx_i=i\n",
    "            maxx_j=j\n",
    "best_C = CC[maxx_j]\n",
    "print(\"    Best parameter C is:\", best_C, \"\\n\", \n",
    "      \"    with method:\",method[maxx_i],\"best f1-score =\",maxx)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method rus and c = 0.01 is: 0.5763873011570309\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method rus and c = 0.1 is: 0.5987956875546934\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method rus and c = 1 is: 0.6025844375210536\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method rus and c = 10 is: 0.6025844375210536\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method ros and c = 0.01 is: 0.6062022430476869\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method ros and c = 0.1 is: 0.6092748532013329\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method ros and c = 1 is: 0.6092748532013329\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method ros and c = 10 is: 0.6092748532013329\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method smote and c = 0.01 is: 0.6024382226402054\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method smote and c = 0.1 is: 0.6060540285018744\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method smote and c = 1 is: 0.6060540285018744\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method smote and c = 10 is: 0.6060540285018744\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method nm and c = 0.01 is: 0.5902498635857195\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method nm and c = 0.1 is: 0.6058579525939121\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method nm and c = 1 is: 0.6067530167184132\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method nm and c = 10 is: 0.6067530167184132\n",
      "[[0.5763873011570309, 0.5987956875546934, 0.6025844375210536, 0.6025844375210536], [0.6062022430476869, 0.6092748532013329, 0.6092748532013329, 0.6092748532013329], [0.6024382226402054, 0.6060540285018744, 0.6060540285018744, 0.6060540285018744], [0.5902498635857195, 0.6058579525939121, 0.6067530167184132, 0.6067530167184132]]\n",
      "--------------\n",
      "-> Result of 4-fold cross validation for logistic regression with our selected features:\n",
      "    Best parameter C is: 0.1 \n",
      "     with method: ros best f1-score = 0.6092748532013329\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CC = [0.01, 0.1, 1, 10]\n",
    "\n",
    "accs = [[0 for i in range(len(CC))]for j in range(4)]\n",
    "folds_num = 4\n",
    "for j in range(0,8,2):\n",
    "    train_sets, validation_sets = four_fold_CV(balanced[j], balanced[j+1], folds_num)\n",
    "\n",
    "    for (cntC, C) in enumerate(CC):\n",
    "        avg_acc = 0\n",
    "        for i in range(folds_num):\n",
    "            try:\n",
    "                t_X = train_sets[i]['X']\n",
    "                t_Y = train_sets[i]['Y']\n",
    "                v_X = validation_sets[i]['X']\n",
    "                v_Y = validation_sets[i]['Y']\n",
    "                clf=Classifier(t_X, t_Y, LogisticRegression(C=C, max_iter=10000))\n",
    "                clf.fit()\n",
    "                avg_acc+=f1_score(v_Y,clf.predict(v_X))\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "        accs[j//2][cntC] = avg_acc / folds_num\n",
    "        print('-> mean f1-score 4-fold cross validation for LogisticRegression with method', method[j//2], \n",
    "              'and c =', C, 'is:', accs[j//2][cntC])\n",
    "\n",
    "\n",
    "print(accs)\n",
    "print(\"--------------\")\n",
    "print(\"-> Result of 4-fold cross validation for logistic regression with our selected features:\")\n",
    "maxx=0\n",
    "maxx_i=-1\n",
    "maxx_j=-1\n",
    "for i in range(len(accs)):\n",
    "    for j in range(len(accs[i])):\n",
    "        if accs[i][j]>maxx:\n",
    "            maxx=accs[i][j]\n",
    "            maxx_i=i\n",
    "            maxx_j=j\n",
    "best_C = CC[maxx_j]\n",
    "print(\"    Best parameter C is:\", best_C, \"\\n\", \n",
    "      \"    with method:\",method[maxx_i],\"best f1-score =\",maxx)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[0.5765120673537338, 0.5823695913009046, 0.5825923605025684, 0.582777659349109], [0.5846216679022144, 0.5864178555777048, 0.5864581939477248, 0.5864913060821646], [0.5834911815785344, 0.5842760118063621, 0.5845090366316807, 0.5844973220431374], [0.5581654481477375, 0.5561945259391313, 0.5504467794440864, 0.549506404407838]]\n",
    "--------------\n",
    "-> Result of 4-fold cross validation for logistic regression with our selected features:\n",
    "    Best parameter C is: 10 \n",
    "     with method: ros best accuracy = 0.5864913060821646\n",
    "[[0.5734392089207982, 0.576941444283601, 0.577563817166687, 0.5773869989578434], [0.5770660722173564, 0.5760060571866957, 0.5773382764772736, 0.5768539684486106], [0.5756844437605892, 0.5751845859666964, 0.5751091243830301, 0.5755657699681545], [0.5977481743060398, 0.5965780219319656, 0.5963515289073948, 0.5954563077004094]]\n",
    "--------------\n",
    "-> Result of 4-fold cross validation for logistic regression :\n",
    "    Best parameter C is: 0.01 \n",
    "     with method: nm best accuracy = 0.5977481743060398"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> mean f1-score 4-fold cross validation for RandomForestClassifier with method rus is: 0.5976990678126525\n",
      "-> mean f1-score 4-fold cross validation for RandomForestClassifier with method ros is: 0.9291436604199588\n",
      "-> mean f1-score 4-fold cross validation for RandomForestClassifier with method smote is: 0.7758391878418495\n",
      "-> mean f1-score 4-fold cross validation for RandomForestClassifier with method nm is: 0.6305172853000833\n",
      "[0.5976990678126525, 0.9291436604199588, 0.7758391878418495, 0.6305172853000833]\n",
      "--------------\n",
      "-> Result of 4-fold cross validation for random forest :\n",
      "with method: ros best f1-score = 0.9291436604199588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accs = [0 for j in range(4)]\n",
    "folds_num = 4\n",
    "for j in range(0,8,2):\n",
    "        train_sets, validation_sets = four_fold_CV(balanced_all_fetuers[j], balanced_all_fetuers[j+1], folds_num)\n",
    "        avg_acc = 0\n",
    "        for i in range(folds_num):\n",
    "            try:\n",
    "                t_X = train_sets[i]['X']\n",
    "                t_Y = train_sets[i]['Y']\n",
    "                v_X = validation_sets[i]['X']\n",
    "                v_Y = validation_sets[i]['Y']\n",
    "                clf=Classifier(t_X, t_Y, RandomForestClassifier(random_state=0))\n",
    "                clf.fit()\n",
    "                avg_acc+=f1_score(v_Y,clf.predict(v_X))\n",
    "            except Exception as e: \n",
    "                    print(e)\n",
    "\n",
    "        accs[j//2] = avg_acc / folds_num\n",
    "        print('-> mean f1-score 4-fold cross validation for RandomForestClassifier with method', method[j//2], \n",
    "              'is:', accs[j//2])\n",
    "\n",
    "\n",
    "print(accs)\n",
    "print(\"--------------\")\n",
    "print(\"-> Result of 4-fold cross validation for random forest :\")\n",
    "c_ind = np.argmax(accs)\n",
    "print(\"with method:\",method[c_ind],\"best f1-score =\",max(accs))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method rus is: 0.58057935575151\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method ros is: 0.8545836828489337\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method smote is: 0.7613718857268211\n",
      "-> mean f1-score 4-fold cross validation for LogisticRegression with method nm is: 0.5791174608431623\n",
      "[0.58057935575151, 0.8545836828489337, 0.7613718857268211, 0.5791174608431623]\n",
      "--------------\n",
      "-> Result of 4-fold cross validation for random forest :\n",
      "with method: ros best accuracy = 0.8545836828489337\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accs = [0 for j in range(4)]\n",
    "folds_num = 4\n",
    "for j in range(0,8,2):\n",
    "        train_sets, validation_sets = four_fold_CV(balanced[j], balanced[j+1], folds_num)\n",
    "        avg_acc = 0\n",
    "        for i in range(folds_num):\n",
    "            try:\n",
    "                t_X = train_sets[i]['X']\n",
    "                t_Y = train_sets[i]['Y']\n",
    "                v_X = validation_sets[i]['X']\n",
    "                v_Y = validation_sets[i]['Y']\n",
    "                clf=Classifier(t_X, t_Y,RandomForestClassifier(random_state=0))\n",
    "                clf.fit()\n",
    "                avg_acc+=f1_score(v_Y,clf.predict(v_X))\n",
    "            except Exception as e: \n",
    "                    print(e)\n",
    "\n",
    "        accs[j//2] = avg_acc / folds_num\n",
    "        print('-> mean f1-score 4-fold cross validation for LogisticRegression with method', method[j//2], \n",
    "              'is:', accs[j//2])\n",
    "\n",
    "\n",
    "print(accs)\n",
    "print(\"--------------\")\n",
    "print(\"-> Result of 4-fold cross validation for random forest with selected features:\")\n",
    "c_ind = np.argmax(accs)\n",
    "print(\"with method:\",method[c_ind],\"best accuracy =\",max(accs))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.5982587020441167, 0.7205548903150465, 0.7321080204319267, 0.7119640286509841]\n",
    "--------------\n",
    "-> Result of 4-fold cross validation for random forest with our selected features:\n",
    "with method: smote best accuracy = 0.7321080204319267\n",
    "\n",
    "[0.58910022555684, 0.8691357481897046, 0.7831332783301392, 0.6622645900004881]\n",
    "--------------\n",
    "-> Result of 4-fold cross validation for random forest :\n",
    "with method: ros best accuracy = 0.8691357481897046"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatemeh\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:06:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:06:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:06:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:06:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-> mean f1-score 4-fold cross validation for XGBClassifier with method rus is: 0.586954141985848\n",
      "[22:06:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatemeh\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:06:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:06:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:07:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-> mean f1-score 4-fold cross validation for XGBClassifier with method ros is: 0.7293677023165749\n",
      "[22:07:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatemeh\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:07:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:07:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:07:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-> mean f1-score 4-fold cross validation for XGBClassifier with method smote is: 0.7100597789238049\n",
      "[22:07:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatemeh\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:07:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:07:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:07:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-> mean f1-score 4-fold cross validation for XGBClassifier with method nm is: 0.6581494567112133\n",
      "[0.586954141985848, 0.7293677023165749, 0.7100597789238049, 0.6581494567112133]\n",
      "--------------\n",
      "-> Result of 4-fold cross validation for XGBClassifier:\n",
      "with method: ros best accuracy = 0.7293677023165749\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accs = [0 for j in range(4)]\n",
    "folds_num = 4\n",
    "for j in range(0,8,2):\n",
    "        train_sets, validation_sets = four_fold_CV(balanced_all_fetuers[j], balanced_all_fetuers[j+1], folds_num)\n",
    "        avg_acc = 0\n",
    "        for i in range(folds_num):\n",
    "            try:\n",
    "                t_X = train_sets[i]['X']\n",
    "                t_Y = train_sets[i]['Y']\n",
    "                v_X = validation_sets[i]['X']\n",
    "                v_Y = validation_sets[i]['Y']\n",
    "                clf=Classifier(t_X, t_Y, XGBClassifier())\n",
    "                clf.fit()\n",
    "                avg_acc+=f1_score(v_Y,clf.predict(v_X))\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "        accs[j//2] = avg_acc / folds_num\n",
    "        print('-> mean f1-score 4-fold cross validation for XGBClassifier with method', method[j//2], \n",
    "              'is:', accs[j//2])\n",
    "\n",
    "\n",
    "print(accs)\n",
    "print(\"--------------\")\n",
    "print(\"-> Result of 4-fold cross validation for XGBClassifier:\")\n",
    "c_ind = np.argmax(accs)\n",
    "print(\"with method:\",method[c_ind],\"best accuracy =\",max(accs))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatemeh\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:14:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:14:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:14:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:14:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-> mean f1-score 4-fold cross validation for XGBClassifier with method rus is: 0.5800582126988227\n",
      "[22:14:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatemeh\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:14:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:14:34] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:14:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-> mean f1-score 4-fold cross validation for XGBClassifier with method ros is: 0.6884348746023645\n",
      "[22:14:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatemeh\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:14:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:14:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:14:42] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-> mean f1-score 4-fold cross validation for XGBClassifier with method smote is: 0.6868460739006393\n",
      "[22:14:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatemeh\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:14:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:14:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[22:14:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "-> mean f1-score 4-fold cross validation for XGBClassifier with method nm is: 0.6399611244747678\n",
      "[0.5800582126988227, 0.6884348746023645, 0.6868460739006393, 0.6399611244747678]\n",
      "--------------\n",
      "-> Result of 4-fold cross validation for XGBClassifier with selected features:\n",
      "with method: ros best accuracy = 0.6884348746023645\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accs = [0 for j in range(4)]\n",
    "folds_num = 4\n",
    "for j in range(0,8,2):\n",
    "        train_sets, validation_sets = four_fold_CV(balanced[j], balanced[j+1], folds_num)\n",
    "        avg_acc = 0\n",
    "        for i in range(folds_num):\n",
    "            try:\n",
    "                t_X = train_sets[i]['X']\n",
    "                t_Y = train_sets[i]['Y']\n",
    "                v_X = validation_sets[i]['X']\n",
    "                v_Y = validation_sets[i]['Y']\n",
    "                clf=Classifier(t_X, t_Y, XGBClassifier())\n",
    "                clf.fit()\n",
    "                avg_acc+=f1_score(v_Y,clf.predict(v_X))\n",
    "            except Exception as e: \n",
    "                print(e)\n",
    "\n",
    "        accs[j//2] = avg_acc / folds_num\n",
    "        print('-> mean f1-score 4-fold cross validation for XGBClassifier with method', method[j//2], \n",
    "              'is:', accs[j//2])\n",
    "\n",
    "\n",
    "print(accs)\n",
    "print(\"--------------\")\n",
    "print(\"-> Result of 4-fold cross validation for XGBClassifier with selected features:\")\n",
    "c_ind = np.argmax(accs)\n",
    "print(\"with method:\",method[c_ind],\"best accuracy =\",max(accs))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0.613850239551532, 0.6784836802586837, 0.7255296747658944, 0.7187145833327385]\n",
    "--------------\n",
    "-> Result of 4-fold cross validation for XGBClassifier with our selected features :\n",
    "with method: smote best accuracy = 0.7255296747658944\n",
    "\n",
    "[0.586101996041527, 0.7164887569980167, 0.7778297949736547, 0.6843311608674746]\n",
    "--------------\n",
    "-> Result of 4-fold cross validation for XGBClassifier :\n",
    "with method: smote best accuracy = 0.7778297949736547\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use 4 ways yo handle imbalanced data:\n",
    "1.Undersampling can be defined as removing some observations of the majority class. This is done until the majority and minority class is balanced out.\n",
    "\n",
    "Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback to undersampling is that we are removing information that may be valuable.\n",
    "2.Undersampling can be defined as removing some observations of the majority class. This is done until the majority and minority class is balanced out.\n",
    "\n",
    "Undersampling can be a good choice when you have a ton of data -think millions of rows. But a drawback to undersampling is that we are removing information that may be valuable.\n",
    "3.SMOTE (Synthetic Minority Oversampling Technique) works by randomly picking a point from the minority class and computing the k-nearest neighbors for this point. The synthetic points are added between the chosen point and its neighbors.\n",
    "SMOTE algorithm works in 4 simple steps:\n",
    "\n",
    "Choose a minority class as the input vector\n",
    "Find its k nearest neighbors (k_neighbors is specified as an argument in the SMOTE() function)\n",
    "Choose one of these neighbors and place a synthetic point anywhere on the line joining the point under consideration and its chosen neighbor\n",
    "Repeat the steps until data is balanced\n",
    "4.NearMiss is an under-sampling technique. Instead of resampling the Minority class, using a distance, this will make the majority class equal to the minority class."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
